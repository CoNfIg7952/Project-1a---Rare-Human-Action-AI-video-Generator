# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16aok9v6YGipom2r1KU-XjwsKW6yt-qqz
"""

!pip install torch torchvision transformers diffusers bitsandbytes imageio av gradiog

import av
import numpy as np
import torch
import gc
import imageio
from transformers import (
    AutoImageProcessor,
    AutoTokenizer,
    VisionEncoderDecoderModel,
    T5EncoderModel,
    BitsAndBytesConfig,
)
from diffusers import LattePipeline
import gradio as gr

# Set up device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Function to clear GPU cache
def flush():
    gc.collect()
    torch.cuda.empty_cache()

# Load models for video captioning
image_processor = AutoImageProcessor.from_pretrained("MCG-NJU/videomae-base")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
caption_model = VisionEncoderDecoderModel.from_pretrained(
    "Neleac/timesformer-gpt2-video-captioning"
).to(device)

# Define the main processing function
def process_video(video_input):
    # Handle Gradio video input
    if isinstance(video_input, dict):
        video_path = video_input["name"]
    else:
        video_path = video_input  # Assume it's a filepath

    # Step 1: Generate caption from the input video
    container = av.open(video_path)

    # Extract evenly spaced frames from the video
    seg_len = container.streams.video[0].frames
    clip_len = caption_model.config.encoder.num_frames
    indices = set(
        np.linspace(0, seg_len - 1, num=clip_len, endpoint=True).astype(np.int64)
    )
    frames = []
    container.seek(0)
    for i, frame in enumerate(container.decode(video=0)):
        if i in indices:
            frames.append(frame.to_ndarray(format="rgb24"))
    container.close()

    # Generate caption
    gen_kwargs = {
        "min_length": 10,
        "max_length": 20,
        "num_beams": 5,
        "no_repeat_ngram_size": 2,
    }
    pixel_values = image_processor(frames, return_tensors="pt").pixel_values.to(device)
    tokens = caption_model.generate(pixel_values, **gen_kwargs)
    caption = tokenizer.decode(tokens[0], skip_special_tokens=True)

    # Step 2: Use the caption to generate a video
    prompt = caption
    negative_prompt = ""

    # Initialize the text encoder
    text_encoder = T5EncoderModel.from_pretrained(
        "maxin-cn/Latte-1",
        subfolder="text_encoder",
        quantization_config=BitsAndBytesConfig(
            load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16
        ),
        device_map="auto",
    )

    # Initialize the pipeline with the text encoder
    pipe = LattePipeline.from_pretrained(
        "maxin-cn/Latte-1",
        text_encoder=text_encoder,
        transformer=None,
        device_map="balanced",
    )

    # Encode the prompt
    with torch.no_grad():
        prompt_embeds, negative_prompt_embeds = pipe.encode_prompt(
            prompt, negative_prompt=negative_prompt
        )

    # Release text encoder and pipeline to free memory
    del text_encoder
    del pipe
    flush()

    # Reload the pipeline without the text encoder
    pipe = LattePipeline.from_pretrained(
        "maxin-cn/Latte-1",
        text_encoder=None,
        torch_dtype=torch.float16,
    ).to(device)

    # Generate the video
    with torch.no_grad():
        videos = pipe(
            video_length=16,
            num_inference_steps=50,
            negative_prompt=None,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            output_type="pt",
        ).frames.cpu()

    # Save the generated video
    output_video_path = "generated_video.mp4"
    videos = (videos.clamp(0, 1) * 255).to(dtype=torch.uint8)
    imageio.mimwrite(
        output_video_path, videos[0].permute(0, 2, 3, 1), fps=8, quality=5
    )

    # Release pipeline and flush memory
    del pipe
    flush()

    # Return both the caption and the generated video path
    return caption, output_video_path

# Set up Gradio Interface
iface = gr.Interface(
    fn=process_video,
    inputs=gr.Video(label="Upload a Video"),
    outputs=[
        gr.Textbox(label="Generated Caption"),
        gr.Video(label="Generated Video"),
    ],
    title="Video Captioning and Generation",
    description="Upload a video to generate a caption and then generate a new video based on the caption.",
)

# Launch the Gradio App
iface.launch(debug=True)

